{
  "id": "21",
  "title": "Filter by Date Range",
  "difficulty": "Easy",
  "category": "Date Functions",
  "description": "Filter records between two dates (inclusive).\n\nYou have a DataFrame with a `date` column. Filter to keep only records where the date is between start_date and end_date (inclusive).",
  "starter_code": "def etl(input_df, start_date, end_date):\n    # Write code here\n    pass",
  "test_setup": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window as W\n\nspark = SparkSession.builder.appName('test').getOrCreate()\n\ndata = [\n    ('2023-01-01', 100),\n    ('2023-01-15', 200),\n    ('2023-02-01', 300),\n    ('2023-03-01', 400)\n]\ninput_df = spark.createDataFrame(data, ['date', 'value'])\nstart_date = '2023-01-10'\nend_date = '2023-02-15'",
  "test_validation": "result = etl(input_df, start_date, end_date)\nassert result is not None, 'Function returned None'\nassert result.count() == 2, f'Expected 2 rows, got {result.count()}'\nprint('âœ… TEST PASSED!')",
  "hints": [
    "Use >= and <= for range",
    "Combine with & operator"
  ],
  "solution": "def etl(input_df, start_date, end_date):\n    return input_df.filter(\n        (F.col('date') >= start_date) & (F.col('date') <= end_date)\n    )"
}
