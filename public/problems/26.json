{
  "id": "26",
  "title": "Drop Null Rows",
  "difficulty": "Easy",
  "category": "Null Handling",
  "description": "Drop rows where any specified column is null.\n\nYou have a DataFrame with columns `col1` and `col2`. Drop any row where either of these columns contains a null value.",
  "starter_code": "def etl(input_df):\n    # Write code here\n    pass",
  "test_setup": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window as W\n\nspark = SparkSession.builder.appName('test').getOrCreate()\n\ndata = [('A', 1), ('B', None), (None, 3), ('D', 4)]\ninput_df = spark.createDataFrame(data, ['col1', 'col2'])",
  "test_validation": "result = etl(input_df)\nassert result is not None, 'Function returned None'\nassert result.count() == 2, f'Expected 2 rows, got {result.count()}'\nprint('âœ… TEST PASSED!')",
  "hints": [
    "dropna() removes null rows",
    "subset limits to specific columns"
  ],
  "solution": "def etl(input_df):\n    return input_df.dropna(subset=['col1', 'col2'])"
}
