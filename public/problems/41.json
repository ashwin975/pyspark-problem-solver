{
  "id": "41",
  "title": "Collect List",
  "difficulty": "Medium",
  "category": "Aggregations",
  "description": "Aggregate values into an array per group.\n\nYou have a DataFrame with `group_id` and `value` columns. Group by group_id and collect all values into an array column called `values`.",
  "starter_code": "def etl(input_df):\n    # Write code here\n    pass",
  "test_setup": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window as W\n\nspark = SparkSession.builder.appName('test').getOrCreate()\n\ndata = [('G1', 'a'), ('G1', 'b'), ('G1', 'c'), ('G2', 'x'), ('G2', 'y')]\ninput_df = spark.createDataFrame(data, ['group_id', 'value'])",
  "test_validation": "result = etl(input_df)\nassert result is not None, 'Function returned None'\nassert 'values' in result.columns, 'Missing values column'\nassert result.count() == 2, f'Expected 2 groups, got {result.count()}'\nprint('âœ… TEST PASSED!')",
  "hints": [
    "F.collect_list() includes duplicates",
    "F.collect_set() for unique values only"
  ],
  "solution": "def etl(input_df):\n    return input_df.groupBy('group_id').agg(\n        F.collect_list('value').alias('values')\n    )"
}
