{
  "id": "32",
  "title": "Pivot Table",
  "difficulty": "Hard",
  "category": "Pivoting",
  "description": "Pivot data to show values as columns.\n\nYou have a DataFrame with `row_key`, `column_key`, and `value` columns. Pivot the data so that unique values of `column_key` become columns, with the sum of `value` as cell values.",
  "starter_code": "def etl(input_df):\n    # Write code here\n    pass",
  "test_setup": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window as W\n\nspark = SparkSession.builder.appName('test').getOrCreate()\n\ndata = [\n    ('R1', 'Q1', 100), ('R1', 'Q2', 150),\n    ('R2', 'Q1', 200), ('R2', 'Q2', 250),\n    ('R1', 'Q1', 50)\n]\ninput_df = spark.createDataFrame(data, ['row_key', 'column_key', 'value'])",
  "test_validation": "result = etl(input_df)\nassert result is not None, 'Function returned None'\nassert 'Q1' in result.columns, 'Missing Q1 column after pivot'\nassert 'Q2' in result.columns, 'Missing Q2 column after pivot'\nprint('âœ… TEST PASSED!')",
  "hints": [
    "GroupBy the row identifier",
    "pivot() on the column to spread",
    "Aggregate the values"
  ],
  "solution": "def etl(input_df):\n    return input_df.groupBy('row_key').pivot('column_key').agg(F.sum('value'))"
}
