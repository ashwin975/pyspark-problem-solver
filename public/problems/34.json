{
  "id": "34",
  "title": "Case When Logic",
  "difficulty": "Easy",
  "category": "Transformations",
  "description": "Create categories based on value ranges.\n\nYou have a DataFrame with a `value` column. Add a `category` column:\n- 'low' if value < 10\n- 'medium' if value >= 10 and < 50\n- 'high' if value >= 50",
  "starter_code": "def etl(input_df):\n    # Write code here\n    pass",
  "test_setup": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window as W\n\nspark = SparkSession.builder.appName('test').getOrCreate()\n\ndata = [(5,), (15,), (50,), (100,)]\ninput_df = spark.createDataFrame(data, ['value'])",
  "test_validation": "result = etl(input_df)\nassert result is not None, 'Function returned None'\nassert 'category' in result.columns, 'Missing category column'\ncats = [row['category'] for row in result.collect()]\nassert 'low' in cats and 'medium' in cats and 'high' in cats, 'Categories not assigned correctly'\nprint('âœ… TEST PASSED!')",
  "hints": [
    "Use F.when() for first condition",
    "Chain .when() for more",
    "End with .otherwise()"
  ],
  "solution": "def etl(input_df):\n    return input_df.withColumn(\n        'category',\n        F.when(F.col('value') < 10, 'low')\n        .when(F.col('value') < 50, 'medium')\n        .otherwise('high')\n    )"
}
