{
  "id": "47",
  "title": "Distinct Count",
  "difficulty": "Easy",
  "category": "Aggregations",
  "description": "Count distinct values per group.\n\nYou have a DataFrame with `category` and `item` columns. Group by category and count the number of unique items as `unique_items`.",
  "starter_code": "def etl(input_df):\n    # Write code here\n    pass",
  "test_setup": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window as W\n\nspark = SparkSession.builder.appName('test').getOrCreate()\n\ndata = [\n    ('A', 'item1'), ('A', 'item2'), ('A', 'item1'),\n    ('B', 'item3'), ('B', 'item3'), ('B', 'item4'), ('B', 'item5')\n]\ninput_df = spark.createDataFrame(data, ['category', 'item'])",
  "test_validation": "result = etl(input_df)\nassert result is not None, 'Function returned None'\nassert 'unique_items' in result.columns, 'Missing unique_items column'\nrows = result.collect()\ncat_a = [r for r in rows if r['category'] == 'A'][0]\nassert cat_a['unique_items'] == 2, f'Category A should have 2 unique items, got {cat_a[\"unique_items\"]}'\nprint('âœ… TEST PASSED!')",
  "hints": [
    "F.count_distinct() or F.countDistinct()",
    "Can pass multiple columns"
  ],
  "solution": "def etl(input_df):\n    return input_df.groupBy('category').agg(\n        F.count_distinct('item').alias('unique_items')\n    )"
}
