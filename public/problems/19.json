{
  "id": "19",
  "title": "Cumulative Sum",
  "difficulty": "Medium",
  "category": "Window Functions",
  "description": "Calculate cumulative sum of sales per product over time.\n\nYou have a DataFrame with columns: product_id, date, amount. Add a `cumulative_sales` column that shows the running total of amount for each product, ordered by date.",
  "starter_code": "def etl(sales_df):\n    # Write code here\n    pass",
  "test_setup": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window as W\n\nspark = SparkSession.builder.appName('test').getOrCreate()\n\ndata = [\n    ('P1', '2023-01-01', 100),\n    ('P1', '2023-01-02', 150),\n    ('P1', '2023-01-03', 200),\n    ('P2', '2023-01-01', 50),\n    ('P2', '2023-01-02', 75)\n]\nsales_df = spark.createDataFrame(data, ['product_id', 'date', 'amount'])",
  "test_validation": "result = etl(sales_df)\nassert result is not None, 'Function returned None'\nassert 'cumulative_sales' in result.columns, 'Missing cumulative_sales column'\nprint('âœ… TEST PASSED!')",
  "hints": [
    "Define window with unboundedPreceding to currentRow",
    "Sum over the window"
  ],
  "solution": "def etl(sales_df):\n    window = W.partitionBy('product_id').orderBy('date').rowsBetween(W.unboundedPreceding, W.currentRow)\n    return sales_df.withColumn('cumulative_sales', F.sum('amount').over(window))"
}
